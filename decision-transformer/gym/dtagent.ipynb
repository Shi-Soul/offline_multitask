{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os \n",
    "import sys\n",
    "from decision_transformer.evaluation.evaluate_episodes import evaluate_episode, evaluate_episode_rtg\n",
    "from decision_transformer.models.decision_transformer import DecisionTransformer\n",
    "from decision_transformer.models.mlp_bc import MLPBCModel\n",
    "from decision_transformer.training.act_trainer import ActTrainer\n",
    "from decision_transformer.training.seq_trainer import SequenceTrainer\n",
    "\n",
    "PWD = os.getcwd()\n",
    "# os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "sys.path.append('../../')\n",
    "from util import *\n",
    "\n",
    "class DTAgent: #TODO: Implement the agent here\n",
    "    def __init__(self, model,\n",
    "                    state_dim, action_dim,\n",
    "                    state_mean=0., state_std=1.,\n",
    "                    scale=1000.,\n",
    "                    rtg=1000,\n",
    "                    device='cuda',\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.mode = 'normal'\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.state_mean = torch.from_numpy(state_mean).to(device=device)\n",
    "        self.state_std = torch.from_numpy(state_std).to(device=device)\n",
    "        self.scale = scale\n",
    "        self.device = device\n",
    "        \n",
    "        self.reset_rtg(rtg)\n",
    "        \n",
    "        \n",
    "    def act(self, state, prev_rew=0 ):\n",
    "        assert state.shape == (self.state_dim,)\n",
    "        # action = np.random.uniform(-5, 5, size=(self.action_dim))\n",
    "        if self.t==0:\n",
    "            self.states = torch.from_numpy(state).reshape(1, self.state_dim).to(device=self.device, dtype=torch.float32)\n",
    "            self.actions = torch.zeros((0, self.action_dim), device=self.device, dtype=torch.float32)\n",
    "            self.rewards = torch.zeros(0, device=self.device, dtype=torch.float32)\n",
    "        else:\n",
    "            cur_state = torch.from_numpy(state).to(device=self.device).reshape(1, self.state_dim)\n",
    "            states = torch.cat([states, cur_state], dim=0)\n",
    "            self.rewards[-1] = prev_rew\n",
    "\n",
    "            if self.mode != 'delayed':\n",
    "                pred_return = target_return[0,-1] - (prev_rew/self.scale)\n",
    "            else:\n",
    "                pred_return = target_return[0,-1]\n",
    "            target_return = torch.cat(\n",
    "                [target_return, pred_return.reshape(1, 1)], dim=1)\n",
    "            timesteps = torch.cat(\n",
    "                [timesteps,\n",
    "                torch.ones((1, 1), device=self.device, dtype=torch.long) * (self.t)], dim=1)\n",
    "            \n",
    "            \n",
    "        self.actions = torch.cat([self.actions, torch.zeros((1, self.action_dim), device=self.device)], dim=0)\n",
    "        self.rewards = torch.cat([self.rewards, torch.zeros(1, device=self.device)])\n",
    "        \n",
    "        action = self.model.get_action(\n",
    "            (self.states.to(dtype=torch.float32) - self.state_mean) / self.state_std,\n",
    "            self.actions.to(dtype=torch.float32),\n",
    "            self.rewards.to(dtype=torch.float32),\n",
    "            self.target_return.to(dtype=torch.float32),\n",
    "            self.timesteps.to(dtype=torch.long),\n",
    "        )\n",
    "        self.actions[-1] = action\n",
    "        action = action.detach().cpu().numpy()\n",
    "        \n",
    "        # state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "\n",
    "        self.t+=1\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def reset_rtg(self,rtg):\n",
    "        self.target_return = torch.tensor(rtg, device=self.device, dtype=torch.float32).reshape(1, 1)\n",
    "        self.timesteps = torch.tensor(0, device=self.device, dtype=torch.long).reshape(1, 1)\n",
    "        self.t = 0\n",
    "        \n",
    "        ...\n",
    "    \n",
    "    \n",
    "\n",
    "def eval_episodes(target_rew, num_eval_episodes, env_walk, env_run, state_dim, act_dim, max_ep_len, scale, state_mean, state_std, device, mode='normal'):\n",
    "    def fn(model):\n",
    "        returns_walk = []\n",
    "        returns_run = []\n",
    "        for _ in range(num_eval_episodes):\n",
    "            with torch.no_grad():\n",
    "                ret_walk, __ = evaluate_episode_rtg(\n",
    "                    env_walk,\n",
    "                    state_dim,\n",
    "                    act_dim,\n",
    "                    model,\n",
    "                    max_ep_len=max_ep_len,\n",
    "                    scale=scale,\n",
    "                    target_return=target_rew/scale,\n",
    "                    mode=mode,\n",
    "                    state_mean=state_mean,\n",
    "                    state_std=state_std,\n",
    "                    device=device,\n",
    "                )\n",
    "                ret_run, __ = evaluate_episode_rtg(\n",
    "                    env_run,\n",
    "                    state_dim,\n",
    "                    act_dim,\n",
    "                    model,\n",
    "                    max_ep_len=max_ep_len,\n",
    "                    scale=scale,\n",
    "                    target_return=target_rew/scale,\n",
    "                    mode=mode,\n",
    "                    state_mean=state_mean,\n",
    "                    state_std=state_std,\n",
    "                    device=device,\n",
    "                )\n",
    "                \n",
    "            returns_walk.append(ret_walk)\n",
    "            returns_run.append(ret_run)\n",
    "        return {\n",
    "            # f'target_{target_rew}_return_mean': np.mean(returns),\n",
    "            # f'target_{target_rew}_return_std': np.std(returns),\n",
    "            f'target_{target_rew}_return_walk_mean': np.mean(returns_walk),\n",
    "            f'target_{target_rew}_return_walk_std': np.std(returns_walk),\n",
    "            f'target_{target_rew}_return_walk_all': returns_walk, \n",
    "            f'target_{target_rew}_return_run_mean': np.mean(returns_run),\n",
    "            f'target_{target_rew}_return_run_std': np.std(returns_run),\n",
    "            f'target_{target_rew}_return_run_all': returns_run,\n",
    "        }\n",
    "    return fn\n",
    "\n",
    "\n",
    "def evaluate_episode_rtg(\n",
    "        env,\n",
    "        state_dim,\n",
    "        act_dim,\n",
    "        model,\n",
    "        max_ep_len=1000,\n",
    "        scale=1000.,\n",
    "        state_mean=0.,\n",
    "        state_std=1.,\n",
    "        device='cuda',\n",
    "        target_return=None,\n",
    "        mode='normal',\n",
    "    ):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device=device)\n",
    "\n",
    "    state_mean = torch.from_numpy(state_mean).to(device=device)\n",
    "    state_std = torch.from_numpy(state_std).to(device=device)\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    if mode == 'noise':\n",
    "        state = state + np.random.normal(0, 0.1, size=state.shape)\n",
    "\n",
    "    # we keep all the histories on the device\n",
    "    # note that the latest action and reward will be \"padding\"\n",
    "    states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
    "    actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n",
    "    rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "\n",
    "    ep_return = target_return\n",
    "    target_return = torch.tensor(ep_return, device=device, dtype=torch.float32).reshape(1, 1)\n",
    "    timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
    "\n",
    "    sim_states = []\n",
    "\n",
    "    episode_return, episode_length = 0, 0\n",
    "    for t in range(max_ep_len):\n",
    "\n",
    "        # add padding\n",
    "        actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n",
    "        rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
    "\n",
    "        action = model.get_action(\n",
    "            (states.to(dtype=torch.float32) - state_mean) / state_std,\n",
    "            actions.to(dtype=torch.float32),\n",
    "            rewards.to(dtype=torch.float32),\n",
    "            target_return.to(dtype=torch.float32),\n",
    "            timesteps.to(dtype=torch.long),\n",
    "        )\n",
    "        actions[-1] = action\n",
    "        action = action.detach().cpu().numpy()\n",
    "\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        # print('step done!!!')\n",
    "\n",
    "        cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n",
    "        states = torch.cat([states, cur_state], dim=0)\n",
    "        rewards[-1] = reward\n",
    "\n",
    "        if mode != 'delayed':\n",
    "            pred_return = target_return[0,-1] - (reward/scale)\n",
    "        else:\n",
    "            pred_return = target_return[0,-1]\n",
    "        target_return = torch.cat(\n",
    "            [target_return, pred_return.reshape(1, 1)], dim=1)\n",
    "        timesteps = torch.cat(\n",
    "            [timesteps,\n",
    "             torch.ones((1, 1), device=device, dtype=torch.long) * (t+1)], dim=1)\n",
    "\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return episode_return, episode_length\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task_bit = 1\n",
    "\n",
    "env_walk = get_gym_env('walk',seed=1, ADD_TASKBIT=task_bit)\n",
    "env_run = get_gym_env('run',seed=1, ADD_TASKBIT=task_bit)\n",
    "max_ep_len = 1000\n",
    "env_targets = [0.8,1,1000, 800, 500, 300, 200]  # evaluation conditioning targets\n",
    "scale = 1000.  # normalization for rewards/returns\n",
    "\n",
    "state_dim = env_walk.observation_space.shape[0]\n",
    "act_dim = env_walk.action_space.shape[0]\n",
    "\n",
    "mode = 'normal'\n",
    "K= 20\n",
    "variant={\n",
    "    'embed_dim': 256,\n",
    "    'n_layer': 3,\n",
    "    'n_head': 1,\n",
    "    'activation_function': 'relu',\n",
    "    'dropout': 0.1,\n",
    "    'USE_DATASET_STR': '__all__',\n",
    "    'task_bit':task_bit,\n",
    "}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dt_model = DecisionTransformer(\n",
    "            state_dim=state_dim,\n",
    "            act_dim=act_dim,\n",
    "            max_length=K,\n",
    "            max_ep_len=max_ep_len,\n",
    "            hidden_size=variant['embed_dim'],\n",
    "            n_layer=variant['n_layer'],\n",
    "            n_head=variant['n_head'],\n",
    "            n_inner=4*variant['embed_dim'],\n",
    "            activation_function=variant['activation_function'],\n",
    "            n_positions=1024,\n",
    "            resid_pdrop=variant['dropout'],\n",
    "            attn_pdrop=variant['dropout'],\n",
    "        )\n",
    "dt_model.load(\"/home/wjxie/wjxie/env/offline_multitask/ckpt/_dt/9.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/NAS2020/Workspaces/DRLGroup/wjxie/env/offline_multitask/decision-transformer/gym\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:00<00:00, 367.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Data Loaded! ###########\n",
      "state_mean: [ 9.31270788e-01 -6.56689485e-02  7.83823628e-01 -3.48372718e-01\n",
      "  5.37133719e-01  6.69629078e-01  4.76454052e-01  4.68625933e-01\n",
      "  8.82615255e-01  7.20799534e-02  5.24125645e-01  7.15013248e-01\n",
      "  4.73387288e-01  2.81045324e-01  1.09207265e+00 -9.45097368e-03\n",
      "  1.59500805e+00 -1.04131104e-03 -1.72432532e-02  1.78307676e-02\n",
      "  3.09820980e-02 -3.02996599e-02  1.19112037e-02  5.32888793e-03\n",
      "  5.00000000e-01]\n",
      "state_std: [0.22458547 0.27925738 0.29520055 0.42085037 0.40492277 0.31483846\n",
      " 0.59775127 0.44280557 0.26616778 0.38072404 0.38143853 0.26182876\n",
      " 0.69636542 0.46042971 0.20927246 1.59334934 0.62619761 4.48222262\n",
      " 9.71117892 6.18426965 8.2479309  9.39151823 6.45248379 7.34752834\n",
      " 0.500001  ]\n"
     ]
    }
   ],
   "source": [
    "from experiment_dmc import from_datasetstr_to_datasetfilepath, make_trajs, read_data\n",
    "dataset_file_paths = from_datasetstr_to_datasetfilepath(variant['USE_DATASET_STR'])\n",
    "trajectories = make_trajs(dataset_file_paths)\n",
    "states, traj_lens, returns = read_data(trajectories,mode,variant['task_bit'])\n",
    "traj_lens, returns = np.array(traj_lens), np.array(returns)\n",
    "print('########### Data Loaded! ###########')\n",
    "\n",
    "# used for input normalization\n",
    "states = np.concatenate(states, axis=0)\n",
    "state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "print('state_mean:', state_mean)\n",
    "print('state_std:', state_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/NAS2020/Workspaces/DRLGroup/wjxie/anaconda3/envs/rlp2/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "for target_return in env_targets:\n",
    "    pprint(eval_episodes(target_return, 20, env_walk, env_run, state_dim, act_dim, max_ep_len, scale, state_mean, state_std, 'cuda', mode=mode)(dt_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
